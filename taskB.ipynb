{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ihZoPQo4oR6",
        "outputId": "1fdde0c2-7f09-4373-a069-7ea08e523f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTav7JMCAwL6",
        "outputId": "6bbf1898-f6a7-438c-ce83-014d8f78c8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=e0cafff4cadfe992ae9630b1f58d1c2ecc14ad56d7242cf4396d89bbd20a7208\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 54473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ed357eeac49f>:294: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
            "<ipython-input-4-ed357eeac49f>:302: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01: Train Loss 1.833 | Val Loss 1.729 | Time 477.03s\n",
            "Epoch 02: Train Loss 1.629 | Val Loss 1.696 | Time 496.12s\n",
            "Epoch 03: Train Loss 1.353 | Val Loss 1.358 | Time 481.90s\n",
            "Epoch 04: Train Loss 1.112 | Val Loss 1.174 | Time 476.16s\n",
            "Epoch 05: Train Loss 0.981 | Val Loss 1.141 | Time 480.35s\n",
            "Epoch 06: Train Loss 0.860 | Val Loss 1.029 | Time 481.64s\n",
            "Epoch 07: Train Loss 0.756 | Val Loss 1.012 | Time 480.40s\n",
            "Epoch 08: Train Loss 0.669 | Val Loss 0.953 | Time 477.56s\n",
            "Epoch 09: Train Loss 0.592 | Val Loss 0.940 | Time 482.45s\n",
            "Epoch 10: Train Loss 0.539 | Val Loss 0.958 | Time 481.69s\n",
            "ROUGE-1: 0.6387 | ROUGE-2: 0.3545 | ROUGE-L: 0.6387\n",
            "\n",
            "--- Test‑Set GT vs Pred ---\n",
            "Example 001\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 002\n",
            "  GT : <unk> High <unk> <unk>\n",
            "  Pred: <bos> <unk> High School\n",
            "\n",
            "Example 003\n",
            "  GT : Minnesota <unk> <unk>\n",
            "  Pred: <bos> West <unk> offense\n",
            "\n",
            "Example 004\n",
            "  GT : List of people from Louisiana\n",
            "  Pred: <bos> List of people from <unk>\n",
            "\n",
            "Example 005\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 006\n",
            "  GT : FC <unk> <unk>\n",
            "  Pred: <bos> <unk> F.C.\n",
            "\n",
            "Example 007\n",
            "  GT : <unk> Maryland\n",
            "  Pred: <bos> <unk> <unk> Wisconsin\n",
            "\n",
            "Example 008\n",
            "  GT : University of <unk>\n",
            "  Pred: <bos> University of <unk>\n",
            "\n",
            "Example 009\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 010\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 011\n",
            "  GT : Arthur <unk>\n",
            "  Pred: <bos> Larry <unk>\n",
            "\n",
            "Example 012\n",
            "  GT : List of The King of <unk> <unk>\n",
            "  Pred: <bos> Culture of <unk>\n",
            "\n",
            "Example 013\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> Jim <unk>\n",
            "\n",
            "Example 014\n",
            "  GT : <unk> Minnesota\n",
            "  Pred: <bos> <unk> Minnesota\n",
            "\n",
            "Example 015\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 016\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 017\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 018\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 019\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 020\n",
            "  GT : <unk> <unk> <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 021\n",
            "  GT : <unk> California\n",
            "  Pred: <bos> <unk> California\n",
            "\n",
            "Example 022\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 023\n",
            "  GT : Peter <unk> (footballer)\n",
            "  Pred: <bos> Peter <unk>\n",
            "\n",
            "Example 024\n",
            "  GT : Chris <unk> (footballer)\n",
            "  Pred: <bos> Chris <unk>\n",
            "\n",
            "Example 025\n",
            "  GT : <unk> du <unk> <unk> of Lake <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 026\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk> Minnesota\n",
            "\n",
            "Example 027\n",
            "  GT : <unk> J. <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 028\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 029\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 030\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 031\n",
            "  GT : <unk> NFL <unk>\n",
            "  Pred: <bos> NFL <unk>\n",
            "\n",
            "Example 032\n",
            "  GT : <unk> California\n",
            "  Pred: <bos> <unk> California\n",
            "\n",
            "Example 033\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 034\n",
            "  GT : John <unk>\n",
            "  Pred: <bos> John <unk>\n",
            "\n",
            "Example 035\n",
            "  GT : <unk> Wisconsin\n",
            "  Pred: <bos> <unk> Wisconsin\n",
            "\n",
            "Example 036\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 037\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 038\n",
            "  GT : <unk> national football team\n",
            "  Pred: <bos> <unk> national football team\n",
            "\n",
            "Example 039\n",
            "  GT : Thomas <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 040\n",
            "  GT : Ray <unk>\n",
            "  Pred: <bos> <unk> <unk> <unk>\n",
            "\n",
            "Example 041\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 042\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 043\n",
            "  GT : Paul Thompson\n",
            "  Pred: <bos> Paul Williams\n",
            "\n",
            "Example 044\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 045\n",
            "  GT : Mark Thompson (footballer)\n",
            "  Pred: <bos> Mark <unk>\n",
            "\n",
            "Example 046\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 047\n",
            "  GT : <unk> (disambiguation)\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 048\n",
            "  GT : May 1\n",
            "  Pred: <bos> May 13\n",
            "\n",
            "Example 049\n",
            "  GT : New <unk> <unk>\n",
            "  Pred: <bos> New <unk> <unk>\n",
            "\n",
            "Example 050\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 051\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 052\n",
            "  GT : North <unk> Pennsylvania\n",
            "  Pred: <bos> East <unk> Pennsylvania\n",
            "\n",
            "Example 053\n",
            "  GT : <unk> <unk> Beach, Florida\n",
            "  Pred: <bos> <unk> <unk> Florida\n",
            "\n",
            "Example 054\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 055\n",
            "  GT : Martin <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 056\n",
            "  GT : <unk> Football Hall of <unk>\n",
            "  Pred: <bos> <unk> Football\n",
            "\n",
            "Example 057\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 058\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 059\n",
            "  GT : <unk> and <unk> School\n",
            "  Pred: <bos> <unk> School School\n",
            "\n",
            "Example 060\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 061\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 062\n",
            "  GT : <unk> (disambiguation)\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 063\n",
            "  GT : Lee <unk> (footballer)\n",
            "  Pred: <bos> Bobby <unk>\n",
            "\n",
            "Example 064\n",
            "  GT : <unk> (disambiguation)\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 065\n",
            "  GT : The <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 066\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 067\n",
            "  GT : San <unk> <unk>\n",
            "  Pred: <bos> San <unk> <unk>\n",
            "\n",
            "Example 068\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk> <unk>\n",
            "\n",
            "Example 069\n",
            "  GT : <unk> national football team\n",
            "  Pred: <bos> <unk> national football team\n",
            "\n",
            "Example 070\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 071\n",
            "  GT : <unk> Academy\n",
            "  Pred: <bos> <unk> <unk> High School\n",
            "\n",
            "Example 072\n",
            "  GT : Paul <unk>\n",
            "  Pred: <bos> Paul <unk>\n",
            "\n",
            "Example 073\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 074\n",
            "  GT : John <unk>\n",
            "  Pred: <bos> John <unk>\n",
            "\n",
            "Example 075\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 076\n",
            "  GT : List of <unk> College people\n",
            "  Pred: <bos> List of <unk>\n",
            "\n",
            "Example 077\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 078\n",
            "  GT : <unk> multiplayer online game\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 079\n",
            "  GT : <unk> van <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 080\n",
            "  GT : <unk> Town F.C.\n",
            "  Pred: <bos> <unk> Town F.C.\n",
            "\n",
            "Example 081\n",
            "  GT : <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 082\n",
            "  GT : <unk> Illinois\n",
            "  Pred: <bos> <unk> Illinois\n",
            "\n",
            "Example 083\n",
            "  GT : Kevin <unk>\n",
            "  Pred: <bos> Kevin <unk>\n",
            "\n",
            "Example 084\n",
            "  GT : <unk> <unk> Wisconsin\n",
            "  Pred: <bos> <unk> <unk> Minnesota\n",
            "\n",
            "Example 085\n",
            "  GT : Indiana <unk> University\n",
            "  Pred: <bos> East <unk> University\n",
            "\n",
            "Example 086\n",
            "  GT : <unk> Hill <unk>\n",
            "  Pred: <bos> <unk> Hill\n",
            "\n",
            "Example 087\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 088\n",
            "  GT : <unk> Ohio\n",
            "  Pred: <bos> <unk> Ohio\n",
            "\n",
            "Example 089\n",
            "  GT : <unk> <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 090\n",
            "  GT : Mark <unk>\n",
            "  Pred: <bos> Mark <unk>\n",
            "\n",
            "Example 091\n",
            "  GT : <unk> New Jersey\n",
            "  Pred: <bos> <unk> New Jersey\n",
            "\n",
            "Example 092\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk> <unk>\n",
            "\n",
            "Example 093\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> <unk>\n",
            "\n",
            "Example 094\n",
            "  GT : <unk> Pennsylvania\n",
            "  Pred: <bos> <unk> Pennsylvania\n",
            "\n",
            "Example 095\n",
            "  GT : <unk> Alabama\n",
            "  Pred: <bos> <unk> Alabama\n",
            "\n",
            "Example 096\n",
            "  GT : <unk> University of Pennsylvania\n",
            "  Pred: <bos> <unk> University\n",
            "\n",
            "Example 097\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> Paul <unk>\n",
            "\n",
            "Example 098\n",
            "  GT : <unk> <unk>\n",
            "  Pred: <bos> Paul <unk>\n",
            "\n",
            "Example 099\n",
            "  GT : <unk> State\n",
            "  Pred: <bos> <unk> State University\n",
            "\n",
            "Example 100\n",
            "  GT : <unk> <unk> Nebraska\n",
            "  Pred: <bos> St. <unk> <unk>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Set environment variable for CUDA memory fragmentation mitigation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "!pip install rouge-score\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "#############################################\n",
        "# 1. Load Preprocessed Data and Build Vocabulary\n",
        "#############################################\n",
        "\n",
        "# Load CSV files (ensure these paths are correct)\n",
        "train_df = pd.read_csv('/content/train_processed.csv')\n",
        "val_df   = pd.read_csv('/content/val_processed.csv')\n",
        "test_df  = pd.read_csv('/content/test_processed.csv')\n",
        "\n",
        "# Build a vocabulary from the training set (from both text and title)\n",
        "min_freq = 0.001 * len(train_df)\n",
        "counter = Counter()\n",
        "\n",
        "def tokenize(text):\n",
        "    # Data is preprocessed (punctuation removed, etc.)\n",
        "    return text.split()\n",
        "\n",
        "for text in train_df['text']:\n",
        "    counter.update(tokenize(text))\n",
        "for title in train_df['title']:\n",
        "    counter.update(tokenize(title))\n",
        "\n",
        "# Initialize vocabulary with special tokens\n",
        "vocab = {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "for token, freq in counter.items():\n",
        "    if freq >= min_freq:\n",
        "        vocab[token] = len(vocab)\n",
        "\n",
        "# Inverse vocabulary mapping for decoding\n",
        "inv_vocab = {idx: token for token, idx in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary Size:\", vocab_size)\n",
        "\n",
        "def text_to_sequence(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "\n",
        "def sequence_to_text(seq, inv_vocab):\n",
        "    tokens = [inv_vocab.get(idx, '<unk>') for idx in seq]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "#############################################\n",
        "# 2. Create Dataset and DataLoader\n",
        "#############################################\n",
        "\n",
        "class TitleDataset(Dataset):\n",
        "    def __init__(self, df, vocab):\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        input_seq = text_to_sequence(row['text'], self.vocab)\n",
        "        target_seq = text_to_sequence(row['title'], self.vocab)\n",
        "        # Add <bos> and <eos> tokens to target sequence\n",
        "        target_seq = [self.vocab['<bos>']] + target_seq + [self.vocab['<eos>']]\n",
        "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    input_lengths = [len(seq) for seq in inputs]\n",
        "    target_lengths = [len(seq) for seq in targets]\n",
        "    max_input = max(input_lengths)\n",
        "    max_target = max(target_lengths)\n",
        "    padded_inputs = [F.pad(seq, (0, max_input - len(seq)), value=vocab['<pad>']) for seq in inputs]\n",
        "    padded_targets = [F.pad(seq, (0, max_target - len(seq)), value=vocab['<pad>']) for seq in targets]\n",
        "    return torch.stack(padded_inputs), torch.stack(padded_targets)\n",
        "\n",
        "# Use a smaller batch size to save memory\n",
        "train_dataset = TitleDataset(train_df, vocab)\n",
        "val_dataset   = TitleDataset(val_df, vocab)\n",
        "test_dataset  = TitleDataset(test_df, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "#############################################\n",
        "# 3. Define Model Components\n",
        "#############################################\n",
        "\n",
        "# --- Basic EncoderRNN (bidirectional GRU) ---\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.2):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        embedded = self.dropout(self.embedding(input_seq))\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        # Concatenate forward and backward hidden states (assumes one layer)\n",
        "        hidden_cat = torch.cat((hidden[0:1], hidden[1:2]), dim=2)\n",
        "        return outputs, hidden_cat\n",
        "\n",
        "    # Setter for loading pre-trained embeddings (e.g., GloVe)\n",
        "    def load_embeddings(self, embeddings):\n",
        "        self.embedding.weight.data.copy_(embeddings)\n",
        "        # Optionally freeze embeddings:\n",
        "        # self.embedding.weight.requires_grad = False\n",
        "\n",
        "# --- Basic DecoderRNN (unidirectional GRU) ---\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.2):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_token, hidden):\n",
        "        input_token = input_token.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.fc(output.squeeze(1))\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "# --- Hierarchical Encoder (HierEncoderRNN) ---\n",
        "class HierEncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, word_hidden_dim, sent_hidden_dim, dropout=0.2):\n",
        "        super(HierEncoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.word_gru = nn.GRU(embed_dim, word_hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.sent_gru = nn.GRU(word_hidden_dim*2, sent_hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        embedded = self.dropout(self.embedding(input_seq))\n",
        "        word_outputs, _ = self.word_gru(embedded)\n",
        "        # For simplicity, treat the entire article as one sentence by averaging word outputs\n",
        "        sent_embedding = torch.mean(word_outputs, dim=1, keepdim=True)\n",
        "        sent_outputs, sent_hidden = self.sent_gru(sent_embedding)\n",
        "        return sent_outputs, sent_hidden\n",
        "\n",
        "# --- Decoder with 2 GRUs (Decoder2RNN) ---\n",
        "class Decoder2RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.2):\n",
        "        super(Decoder2RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru1 = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_token, hidden):\n",
        "        input_token = input_token.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        out1, hidden1 = self.gru1(embedded, hidden)\n",
        "        out2, hidden2 = self.gru2(out1, hidden)\n",
        "        output = self.fc(out2.squeeze(1))\n",
        "        output = F.log_softmax(output, dim=1)\n",
        "        return output, hidden2\n",
        "\n",
        "# --- Seq2seq Model (selectable encoder/decoder) ---\n",
        "class Seq2seqRNN(nn.Module):\n",
        "    def __init__(self, encoder, decoder, bos_token_idx, eos_token_idx, max_new_tokens=20):\n",
        "        super(Seq2seqRNN, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.bos_token_idx = bos_token_idx\n",
        "        self.eos_token_idx = eos_token_idx\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "    def forward(self, input_seq, target_seq=None, teacher_forcing_ratio=0.5, use_beam_search=False, beam_width=3):\n",
        "        batch_size = input_seq.size(0)\n",
        "        encoder_outputs, hidden = self.encoder(input_seq)\n",
        "        decoder_input = torch.full((batch_size,), self.bos_token_idx, dtype=torch.long, device=input_seq.device)\n",
        "        outputs = []\n",
        "        if target_seq is not None:\n",
        "            target_length = target_seq.size(1)\n",
        "            for t in range(target_length):\n",
        "                decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
        "                outputs.append(decoder_output.unsqueeze(1))\n",
        "                if torch.rand(1).item() < teacher_forcing_ratio:\n",
        "                    decoder_input = target_seq[:, t]\n",
        "                else:\n",
        "                    decoder_input = decoder_output.argmax(dim=1)\n",
        "            outputs = torch.cat(outputs, dim=1)\n",
        "        else:\n",
        "            if not use_beam_search:\n",
        "                generated_tokens = []\n",
        "                for _ in range(self.max_new_tokens):\n",
        "                    decoder_output, hidden = self.decoder(decoder_input, hidden)\n",
        "                    top1 = decoder_output.argmax(dim=1)\n",
        "                    generated_tokens.append(top1.unsqueeze(1))\n",
        "                    decoder_input = top1\n",
        "                    if (top1 == self.eos_token_idx).all():\n",
        "                        break\n",
        "                outputs = torch.cat(generated_tokens, dim=1)\n",
        "            else:\n",
        "                outputs = self.beam_search_decoding(decoder_input, hidden, beam_width)\n",
        "        return outputs\n",
        "\n",
        "    def beam_search_decoding(self, decoder_input, hidden, beam_width):\n",
        "        batch_size = decoder_input.size(0)\n",
        "        all_outputs = []\n",
        "        for i in range(batch_size):\n",
        "            beams = [([self.bos_token_idx], 0.0, hidden[:, i:i+1, :])]\n",
        "            for _ in range(self.max_new_tokens):\n",
        "                new_beams = []\n",
        "                for seq, score, hidden_i in beams:\n",
        "                    last_token = torch.tensor([seq[-1]], device=hidden.device)\n",
        "                    decoder_output, hidden_new = self.decoder(last_token, hidden_i)\n",
        "                    topv, topi = decoder_output.topk(beam_width)\n",
        "                    for k in range(beam_width):\n",
        "                        token = topi[0][k].item()\n",
        "                        new_seq = seq + [token]\n",
        "                        new_score = score + topv[0][k].item()\n",
        "                        new_beams.append((new_seq, new_score, hidden_new))\n",
        "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "                if all(seq[-1] == self.eos_token_idx for seq, _, _ in beams):\n",
        "                    break\n",
        "            best_seq = beams[0][0]\n",
        "            all_outputs.append(torch.tensor(best_seq[1:], device=hidden.device))\n",
        "        max_len = max(seq.size(0) for seq in all_outputs)\n",
        "        padded = []\n",
        "        for seq in all_outputs:\n",
        "            if seq.size(0) < max_len:\n",
        "                pad = torch.full((max_len - seq.size(0),), self.eos_token_idx, device=seq.device)\n",
        "                seq = torch.cat([seq, pad])\n",
        "            padded.append(seq.unsqueeze(0))\n",
        "        return torch.cat(padded, dim=0)\n",
        "\n",
        "#############################################\n",
        "# 4. Instantiate Model and (Optionally) Load GloVe\n",
        "#############################################\n",
        "\n",
        "embed_dim   = 300\n",
        "hidden_dim  = 300\n",
        "bos_token_idx = vocab['<bos>']\n",
        "eos_token_idx = vocab['<eos>']\n",
        "\n",
        "# You can choose your variant:\n",
        "# Basic:\n",
        "# encoder = EncoderRNN(vocab_size, embed_dim, hidden_dim)\n",
        "# decoder = DecoderRNN(vocab_size, embed_dim, hidden_dim)\n",
        "\n",
        "# Improved (hierarchical encoder and 2-GRU decoder)\n",
        "encoder = HierEncoderRNN(vocab_size, embed_dim, word_hidden_dim=hidden_dim, sent_hidden_dim=hidden_dim)\n",
        "decoder = Decoder2RNN(vocab_size, embed_dim, hidden_dim)\n",
        "\n",
        "model = Seq2seqRNN(encoder, decoder, bos_token_idx, eos_token_idx, max_new_tokens=20)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Optionally load GloVe embeddings\n",
        "def load_glove_embeddings(glove_path, vocab, embed_dim):\n",
        "    embeddings = torch.randn(len(vocab), embed_dim)\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            token = parts[0]\n",
        "            if token in vocab:\n",
        "                vec = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float)\n",
        "                embeddings[vocab[token]] = vec\n",
        "    return embeddings\n",
        "\n",
        "# Uncomment if you have a GloVe file:\n",
        "# glove_path = '/content/glove.6B.300d.txt'\n",
        "# glove_embeddings = load_glove_embeddings(glove_path, vocab, embed_dim)\n",
        "# encoder.load_embeddings(glove_embeddings)\n",
        "\n",
        "#############################################\n",
        "# 5. Training and Evaluation Functions (using Mixed Precision)\n",
        "#############################################\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss(ignore_index=vocab['<pad>'])\n",
        "scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for input_seq, target_seq in dataloader:\n",
        "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(input_seq, target_seq, teacher_forcing_ratio=0.5)\n",
        "            output_dim = output.size(-1)\n",
        "            output = output.view(-1, output_dim)\n",
        "            target_seq = target_seq.view(-1)\n",
        "            loss = criterion(output, target_seq)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        epoch_loss += loss.item()\n",
        "    torch.cuda.empty_cache()  # clear cache after each epoch\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for input_seq, target_seq in dataloader:\n",
        "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "            output = model(input_seq, target_seq, teacher_forcing_ratio=0)\n",
        "            output_dim = output.size(-1)\n",
        "            output = output.view(-1, output_dim)\n",
        "            target_seq = target_seq.view(-1)\n",
        "            loss = criterion(output, target_seq)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch {epoch+1:02}: Train Loss {train_loss:.3f} | Val Loss {val_loss:.3f} | Time {end_time-start_time:.2f}s\")\n",
        "\n",
        "#############################################\n",
        "# 6. Inference and ROUGE Evaluation\n",
        "#############################################\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def generate_title(model, input_seq, device, use_beam_search=False):\n",
        "    model.eval()\n",
        "    input_seq = input_seq.to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_seq = model(input_seq.unsqueeze(0), target_seq=None, use_beam_search=use_beam_search)\n",
        "    generated_seq = generated_seq.squeeze(0).tolist()\n",
        "    if eos_token_idx in generated_seq:\n",
        "        generated_seq = generated_seq[:generated_seq.index(eos_token_idx)]\n",
        "    return sequence_to_text(generated_seq, inv_vocab)\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def evaluate_rouge(model, dataloader, device):\n",
        "    model.eval()\n",
        "    rouge1_total, rouge2_total, rougeL_total = 0, 0, 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for input_seq, target_seq in dataloader:\n",
        "            input_seq = input_seq.to(device)\n",
        "            for i in range(input_seq.size(0)):\n",
        "                gt_seq = target_seq[i].tolist()\n",
        "                gt_seq = [x for x in gt_seq if x not in [vocab['<bos>'], vocab['<eos>'], vocab['<pad>']]]\n",
        "                gt_title = sequence_to_text(gt_seq, inv_vocab)\n",
        "                pred_title = generate_title(model, input_seq[i], device, use_beam_search=False)\n",
        "                scores = scorer.score(gt_title, pred_title)\n",
        "                rouge1_total += scores['rouge1'].fmeasure\n",
        "                rouge2_total += scores['rouge2'].fmeasure\n",
        "                rougeL_total += scores['rougeL'].fmeasure\n",
        "                count += 1\n",
        "    return rouge1_total/count, rouge2_total/count, rougeL_total/count\n",
        "\n",
        "r1, r2, rL = evaluate_rouge(model, test_loader, device)\n",
        "print(f\"ROUGE-1: {r1:.4f} | ROUGE-2: {r2:.4f} | ROUGE-L: {rL:.4f}\")\n",
        "print(\"\\n--- Test‑Set GT vs Pred ---\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, (input_seq, target_seq) in enumerate(test_loader, 1):\n",
        "        input_seq = input_seq.to(device)\n",
        "        pred = generate_title(model, input_seq.squeeze(0), device, use_beam_search=False)\n",
        "\n",
        "        # Recover GT title string\n",
        "        gt_idxs = [i for i in target_seq.squeeze(0).tolist()\n",
        "                   if i not in (vocab['<bos>'], vocab['<eos>'], vocab['<pad>'])]\n",
        "        gt = sequence_to_text(gt_idxs, inv_vocab)\n",
        "\n",
        "        print(f\"Example {idx:03d}\\n  GT : {gt}\\n  Pred: {pred}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "# Mitigate CUDA fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "# Set environment variable for CUDA memory fragmentation mitigation\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "!pip install rouge-score\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "# ------------------------\n",
        "# 1. Runtime Configuration\n",
        "# ------------------------\n",
        "use_glove     = input(\"Use GloVe embeddings? (y/n): \").strip().lower().startswith('y')\n",
        "glove_path = None\n",
        "if use_glove:\n",
        "    glove_path = input(\"→ Path to your GloVe file (e.g. /content/glove.6B.300d.txt): \").strip()\n",
        "    if not os.path.isfile(glove_path):\n",
        "        raise FileNotFoundError(f\"GloVe file not found at '{glove_path}'\")\n",
        "    # Infer embedding dimension from the first line\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        first = f.readline().split()\n",
        "        inferred_dim = len(first) - 1\n",
        "    print(f\"Detected GloVe embedding dimension: {inferred_dim}\")\n",
        "else:\n",
        "    inferred_dim = 300  # default if not using GloVe\n",
        "\n",
        "use_hier      = input(\"Use Hierarchical Encoder? (y/n): \").strip().lower().startswith('y')\n",
        "use_decoder2  = input(\"Use 2‑GRU Decoder? (y/n): \").strip().lower().startswith('y')\n",
        "use_beam      = input(\"Use Beam Search? (y/n): \").strip().lower().startswith('y')\n",
        "beam_width    = int(input(\"Beam width (e.g. 3): \").strip() or 3)\n",
        "\n",
        "print(f\"\\nConfig → GloVe: {use_glove}, Hier: {use_hier}, Decoder2: {use_decoder2}, Beam: {use_beam}, k={beam_width}\\n\")\n",
        "\n",
        "# ------------------------\n",
        "# 2. Load Data & Build Vocab\n",
        "# ------------------------\n",
        "train_df = pd.read_csv('/content/train_processed (1).csv')\n",
        "val_df   = pd.read_csv('/content/val_processed (1).csv')\n",
        "test_df  = pd.read_csv('/content/test_processed (1).csv')\n",
        "\n",
        "def tokenize(txt): return txt.split()\n",
        "\n",
        "min_freq = 0.001 * len(train_df)\n",
        "ctr = Counter()\n",
        "for col in ['text','title']:\n",
        "    for s in train_df[col]:\n",
        "        ctr.update(tokenize(s))\n",
        "\n",
        "vocab = {'<pad>':0,'<bos>':1,'<eos>':2,'<unk>':3}\n",
        "for tok,f in ctr.items():\n",
        "    if f >= min_freq:\n",
        "        vocab[tok] = len(vocab)\n",
        "inv_vocab = {i:t for t,i in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "def text2seq(s):\n",
        "    return [vocab.get(t, vocab['<unk>']) for t in tokenize(s)]\n",
        "def seq2text(seq):\n",
        "    return \" \".join(inv_vocab.get(i,'<unk>') for i in seq)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Dataset & Dataloader\n",
        "# ------------------------\n",
        "class TitleDS(Dataset):\n",
        "    def __init__(self,df): self.df=df\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self,i):\n",
        "        row=self.df.iloc[i]\n",
        "        inp=text2seq(row['text'])\n",
        "        tgt=[vocab['<bos>']] + text2seq(row['title']) + [vocab['<eos>']]\n",
        "        return torch.tensor(inp), torch.tensor(tgt)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ins,ts = zip(*batch)\n",
        "    Mi, Mt = max(len(x) for x in ins), max(len(x) for x in ts)\n",
        "    ins = torch.stack([F.pad(x,(0,Mi-len(x)),value=vocab['<pad>']) for x in ins])\n",
        "    ts  = torch.stack([F.pad(x,(0,Mt-len(x)),value=vocab['<pad>']) for x in ts])\n",
        "    return ins, ts\n",
        "\n",
        "train_loader = DataLoader(TitleDS(train_df), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(TitleDS(val_df),   batch_size=8, shuffle=False,collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(TitleDS(test_df),  batch_size=1, shuffle=False,collate_fn=collate_fn)\n",
        "\n",
        "# ------------------------\n",
        "# 4. Model Components\n",
        "# ------------------------\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        return self.gru(self.emb(x))\n",
        "    def load_embeddings(self,weights):\n",
        "        self.emb.weight.data.copy_(weights)\n",
        "\n",
        "class HierEncoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,wh,sh):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.wgru=nn.GRU(ed,wh,batch_first=True)\n",
        "        self.sgru=nn.GRU(wh,sh,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        wo,wh = self.wgru(self.emb(x))\n",
        "        se = wo.mean(dim=1,keepdim=True)\n",
        "        return self.sgru(se)\n",
        "    def load_embeddings(self,weights):\n",
        "        self.emb.weight.data.copy_(weights)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "        self.fc = nn.Linear(hd,vs)\n",
        "    def forward(self,tk,h):\n",
        "        o,h = self.gru(self.emb(tk.unsqueeze(1)),h)\n",
        "        return F.log_softmax(self.fc(o.squeeze(1)),dim=1), h\n",
        "\n",
        "class Decoder2RNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.g1 = nn.GRU(ed,hd,batch_first=True)\n",
        "        self.g2 = nn.GRU(hd,hd,batch_first=True)\n",
        "        self.fc = nn.Linear(hd,vs)\n",
        "    def forward(self,tk,h):\n",
        "        o1,h1 = self.g1(self.emb(tk.unsqueeze(1)),h)\n",
        "        o2,h2 = self.g2(o1,h1)\n",
        "        return F.log_softmax(self.fc(o2.squeeze(1)),dim=1), h2\n",
        "\n",
        "class Seq2seqRNN(nn.Module):\n",
        "    def __init__(self,enc,dec,bos,eos,mx=20):\n",
        "        super().__init__()\n",
        "        self.enc, self.dec = enc, dec\n",
        "        self.bos, self.eos, self.mx = bos, eos, mx\n",
        "\n",
        "    def forward(self,src,tgt=None,tf=0.5,beam=False,k=3):\n",
        "        bs = src.size(0)\n",
        "        eo,hidden = self.enc(src)\n",
        "        inp = src.new_full((bs,), self.bos)\n",
        "        outputs=[]\n",
        "        if tgt is not None:\n",
        "            for t in range(tgt.size(1)):\n",
        "                out,hidden = self.dec(inp,hidden)\n",
        "                outputs.append(out.unsqueeze(1))\n",
        "                inp = tgt[:,t] if torch.rand(1).item()<tf else out.argmax(1)\n",
        "            return torch.cat(outputs,dim=1)\n",
        "        # inference\n",
        "        if not beam:\n",
        "            seqs=[]\n",
        "            for _ in range(self.mx):\n",
        "                out,hidden = self.dec(inp,hidden)\n",
        "                top1 = out.argmax(1)\n",
        "                seqs.append(top1.unsqueeze(1))\n",
        "                inp = top1\n",
        "                if (top1==self.eos).all(): break\n",
        "            return torch.cat(seqs,dim=1)\n",
        "        # beam search\n",
        "        all_out=[]\n",
        "        for i in range(bs):\n",
        "            beams=[([self.bos],0.0,hidden[:,i:i+1,:])]\n",
        "            for _ in range(self.mx):\n",
        "                nb=[]\n",
        "                for seq,sc,hi in beams:\n",
        "                    last = src.new_tensor([seq[-1]])\n",
        "                    out,hn = self.dec(last,hi)\n",
        "                    vals,inds = out.topk(k)\n",
        "                    for j in range(k):\n",
        "                        nb.append((seq+[inds[0,j].item()], sc+vals[0,j].item(), hn))\n",
        "                beams = sorted(nb, key=lambda x:x[1], reverse=True)[:k]\n",
        "                if all(s[-1]==self.eos for s,_,_ in beams): break\n",
        "            best = beams[0][0][1:]\n",
        "            all_out.append(torch.tensor(best,device=src.device))\n",
        "        ml = max(o.size(0) for o in all_out)\n",
        "        return torch.stack([F.pad(o,(0,ml-o.size(0)),value=self.eos) for o in all_out],dim=0)\n",
        "\n",
        "# ------------------------\n",
        "# 5. Instantiate Model\n",
        "# ------------------------\n",
        "embed_dim = inferred_dim\n",
        "hid_dim   = 300\n",
        "bos, eos  = vocab['<bos>'], vocab['<eos>']\n",
        "\n",
        "# Encoder choice\n",
        "encoder = HierEncoderRNN(vocab_size, embed_dim, hid_dim, hid_dim) if use_hier \\\n",
        "          else EncoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "# Load GloVe if requested\n",
        "if use_glove:\n",
        "    print(\"→ Loading GloVe embeddings from\", glove_path)\n",
        "    def load_glove(path, vs, ed):\n",
        "        E = torch.randn(vs, ed)\n",
        "        with open(path,'r',encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.split()\n",
        "                w, vec = parts[0], torch.tensor(list(map(float,parts[1:])))\n",
        "                idx = vocab.get(w)\n",
        "                if idx is not None:\n",
        "                    E[idx] = vec\n",
        "        return E\n",
        "    encoder.load_embeddings(load_glove(glove_path, vocab_size, embed_dim))\n",
        "\n",
        "# Decoder choice\n",
        "decoder = Decoder2RNN(vocab_size, embed_dim, hid_dim) if use_decoder2 \\\n",
        "          else DecoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model  = Seq2seqRNN(encoder, decoder, bos, eos, mx=20).to(device)\n",
        "\n",
        "# ------------------------\n",
        "# 6. Training & Eval\n",
        "# ------------------------\n",
        "opt    = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "crit   = nn.NLLLoss(ignore_index=vocab['<pad>'])\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def train_epoch():\n",
        "    model.train(); total=0\n",
        "    for src,tgt in train_loader:\n",
        "        src,tgt = src.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(src,tgt,tf=0.5,beam=False)\n",
        "            loss = crit(out.view(-1,out.size(-1)), tgt.view(-1))\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt); scaler.update()\n",
        "        total += loss.item()\n",
        "    return total/len(train_loader)\n",
        "\n",
        "def eval_epoch():\n",
        "    model.eval(); total=0\n",
        "    with torch.no_grad():\n",
        "        for src,tgt in val_loader:\n",
        "            src,tgt = src.to(device), tgt.to(device)\n",
        "            out = model(src,tgt,tf=0,beam=False)\n",
        "            total += crit(out.view(-1,out.size(-1)), tgt.view(-1)).item()\n",
        "    return total/len(val_loader)\n",
        "\n",
        "for ep in range(1,6):\n",
        "    t0 = time.time()\n",
        "    tr = train_epoch(); va = eval_epoch()\n",
        "    print(f\"Epoch {ep}: train {tr:.3f} | val {va:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "# ------------------------\n",
        "# 7. ROUGE Evaluation\n",
        "# ------------------------\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
        "r1=r2=rL=cnt=0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for src,tgt in test_loader:\n",
        "        src = src.to(device)\n",
        "        pred_seq = model(src, tgt=None, beam=use_beam, k=beam_width).squeeze(0).tolist()\n",
        "        if eos in pred_seq: pred_seq = pred_seq[:pred_seq.index(eos)]\n",
        "        pred_txt = seq2text(pred_seq)\n",
        "        tgt_seq = [i for i in tgt.squeeze(0).tolist() if i not in (bos,eos,vocab['<pad>'])]\n",
        "        tgt_txt  = seq2text(tgt_seq)\n",
        "        scr = scorer.score(tgt_txt, pred_txt)\n",
        "        r1 += scr['rouge1'].fmeasure\n",
        "        r2 += scr['rouge2'].fmeasure\n",
        "        rL += scr['rougeL'].fmeasure\n",
        "        cnt += 1\n",
        "\n",
        "print(f\"\\n→ ROUGE on test ({'beam' if use_beam else 'greedy'}):\")\n",
        "print(f\"   ROUGE‑1: {r1/cnt:.4f}\")\n",
        "print(f\"   ROUGE‑2: {r2/cnt:.4f}\")\n",
        "print(f\"   ROUGE‑L: {rL/cnt:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKv4FGy91vyo",
        "outputId": "db95b63b-e1a9-4b2b-9393-e14ec30da2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=308a9e708342dddbbac3c835cf6d89623f7780efb835bdf24f1d45c0f83cc831\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GloVe embeddings? (y/n): n\n",
            "Use Hierarchical Encoder? (y/n): y\n",
            "Use 2‑GRU Decoder? (y/n): n\n",
            "Use Beam Search? (y/n): n\n",
            "Beam width (e.g. 3): 3\n",
            "\n",
            "Config → GloVe: False, Hier: True, Decoder2: False, Beam: False, k=3\n",
            "\n",
            "Vocab size: 54473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-60874a53fac4>:527: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-1-60874a53fac4>:534: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train 1.767 | val 1.539 | 273.1s\n",
            "Epoch 2: train 1.215 | val 1.179 | 272.8s\n",
            "Epoch 3: train 0.906 | val 0.962 | 261.7s\n",
            "Epoch 4: train 0.686 | val 0.919 | 262.9s\n",
            "Epoch 5: train 0.542 | val 0.894 | 260.3s\n",
            "\n",
            "→ ROUGE on test (greedy):\n",
            "   ROUGE‑1: 0.6577\n",
            "   ROUGE‑2: 0.3828\n",
            "   ROUGE‑L: 0.6510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# # Mitigate CUDA fragmentation\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import time\n",
        "# from collections import Counter\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import nltk\n",
        "# import pandas as pd\n",
        "# import re\n",
        "# import nltk\n",
        "# import string\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import time\n",
        "# from collections import Counter\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# !pip install rouge-score\n",
        "# nltk.download('punkt')\n",
        "# from rouge_score import rouge_scorer\n",
        "\n",
        "# # ------------------------\n",
        "# # 1. Runtime Configuration\n",
        "# # ------------------------\n",
        "# use_glove     = input(\"Use GloVe embeddings? (y/n): \").strip().lower().startswith('y')\n",
        "# use_hier      = input(\"Use Hierarchical Encoder? (y/n): \").strip().lower().startswith('y')\n",
        "# use_decoder2  = input(\"Use 2‑GRU Decoder? (y/n): \").strip().lower().startswith('y')\n",
        "# use_beam      = input(\"Use Beam Search? (y/n): \").strip().lower().startswith('y')\n",
        "# beam_width    = int(input(\"Beam width (e.g. 3): \").strip() or 3)\n",
        "\n",
        "# print(f\"\\nConfig → GloVe: {use_glove}, Hier: {use_hier}, Decoder2: {use_decoder2}, Beam: {use_beam}, k={beam_width}\\n\")\n",
        "\n",
        "# # ------------------------\n",
        "# # 2. Load Data & Build Vocab\n",
        "# # ------------------------\n",
        "# train_df = pd.read_csv('/content/train_processed.csv')\n",
        "# val_df   = pd.read_csv('/content/val_processed.csv')\n",
        "# test_df  = pd.read_csv('/content/test_processed.csv')\n",
        "\n",
        "# def tokenize(txt): return txt.split()\n",
        "\n",
        "# # Build freq counter\n",
        "# min_freq = 0.001 * len(train_df)\n",
        "# ctr = Counter()\n",
        "# for col in ['text','title']:\n",
        "#     for s in train_df[col]:\n",
        "#         ctr.update(tokenize(s))\n",
        "\n",
        "# # Build vocab\n",
        "# vocab = {'<pad>':0,'<bos>':1,'<eos>':2,'<unk>':3}\n",
        "# for tok,f in ctr.items():\n",
        "#     if f>=min_freq:\n",
        "#         vocab[tok] = len(vocab)\n",
        "# inv_vocab = {i:t for t,i in vocab.items()}\n",
        "# vocab_size = len(vocab)\n",
        "# print(\"Vocab size:\",vocab_size)\n",
        "\n",
        "# def text2seq(s):\n",
        "#     return [vocab.get(t,4) for t in tokenize(s)]\n",
        "# def seq2text(seq):\n",
        "#     return \" \".join(inv_vocab.get(i,'<unk>') for i in seq)\n",
        "\n",
        "# # ------------------------\n",
        "# # 3. Dataset & Dataloader\n",
        "# # ------------------------\n",
        "# class TitleDS(Dataset):\n",
        "#     def __init__(self,df):\n",
        "#         self.df=df\n",
        "#     def __len__(self): return len(self.df)\n",
        "#     def __getitem__(self,i):\n",
        "#         row=self.df.iloc[i]\n",
        "#         inp=text2seq(row['text'])\n",
        "#         tgt=[vocab['<bos>']]+text2seq(row['title'])+[vocab['<eos>']]\n",
        "#         return torch.tensor(inp), torch.tensor(tgt)\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     ins,ts = zip(*batch)\n",
        "#     li=[len(x) for x in ins]; lt=[len(x) for x in ts]\n",
        "#     Mi, Mt = max(li), max(lt)\n",
        "#     ins = torch.stack([F.pad(x,(0,Mi-len(x)),value=0) for x in ins])\n",
        "#     ts  = torch.stack([F.pad(x,(0,Mt-len(x)),value=0) for x in ts])\n",
        "#     return ins, ts\n",
        "\n",
        "# train_loader = DataLoader(TitleDS(train_df), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "# val_loader   = DataLoader(TitleDS(val_df),   batch_size=8, shuffle=False,collate_fn=collate_fn)\n",
        "# test_loader  = DataLoader(TitleDS(test_df),  batch_size=1, shuffle=False,collate_fn=collate_fn)\n",
        "\n",
        "# # ------------------------\n",
        "# # 4. Model Components\n",
        "# # ------------------------\n",
        "# class EncoderRNN(nn.Module):\n",
        "#     def __init__(self,vs,ed,hd,drop=0.2):\n",
        "#         super().__init__()\n",
        "#         self.emb=nn.Embedding(vs,ed)\n",
        "#         self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "#     def forward(self,x):\n",
        "#         e=self.emb(x)\n",
        "#         o,h=self.gru(e)\n",
        "#         return o,h\n",
        "#     def load_embeddings(self,weights):\n",
        "#         self.emb.weight.data.copy_(weights)\n",
        "\n",
        "# class HierEncoderRNN(nn.Module):\n",
        "#     def __init__(self,vs,ed,wh,sh,drop=0.2):\n",
        "#         super().__init__()\n",
        "#         self.emb=nn.Embedding(vs,ed)\n",
        "#         self.wgru=nn.GRU(ed,wh,batch_first=True)\n",
        "#         self.sgru=nn.GRU(wh,sh,batch_first=True)\n",
        "#     def forward(self,x):\n",
        "#         e=self.emb(x)\n",
        "#         wo,wh=self.wgru(e)\n",
        "#         # average over time → sentence embedding\n",
        "#         se=wo.mean(dim=1,keepdim=True)\n",
        "#         so,sh=self.sgru(se)\n",
        "#         return so,sh\n",
        "#     def load_embeddings(self,weights):\n",
        "#         self.emb.weight.data.copy_(weights)\n",
        "\n",
        "# class DecoderRNN(nn.Module):\n",
        "#     def __init__(self,vs,ed,hd,drop=0.2):\n",
        "#         super().__init__()\n",
        "#         self.emb=nn.Embedding(vs,ed)\n",
        "#         self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "#         self.fc=nn.Linear(hd,vs)\n",
        "#     def forward(self,token,hidden):\n",
        "#         e=self.emb(token.unsqueeze(1))\n",
        "#         o,h=self.gru(e,hidden)\n",
        "#         return F.log_softmax(self.fc(o.squeeze(1)),dim=1), h\n",
        "\n",
        "# class Decoder2RNN(nn.Module):\n",
        "#     def __init__(self,vs,ed,hd,drop=0.2):\n",
        "#         super().__init__()\n",
        "#         self.emb=nn.Embedding(vs,ed)\n",
        "#         self.g1=nn.GRU(ed,hd,batch_first=True)\n",
        "#         self.g2=nn.GRU(hd,hd,batch_first=True)\n",
        "#         self.fc=nn.Linear(hd,vs)\n",
        "#     def forward(self,token,hidden):\n",
        "#         e=self.emb(token.unsqueeze(1))\n",
        "#         o1,h1=self.g1(e,hidden)\n",
        "#         o2,h2=self.g2(o1,h1)\n",
        "#         return F.log_softmax(self.fc(o2.squeeze(1)),dim=1), h2\n",
        "\n",
        "# class Seq2seqRNN(nn.Module):\n",
        "#     def __init__(self,enc,dec,bos,eos,mx=20):\n",
        "#         super().__init__()\n",
        "#         self.enc, self.dec = enc, dec\n",
        "#         self.bos, self.eos, self.mx = bos,eos,mx\n",
        "\n",
        "#     def forward(self,src,tgt=None,tf=0.5,beam=False,k=3):\n",
        "#         bs=src.size(0)\n",
        "#         eo,hidden = self.enc(src)\n",
        "#         inp = src.new_full((bs,),self.bos)\n",
        "#         outputs=[]\n",
        "#         if tgt is not None:\n",
        "#             L=tgt.size(1)\n",
        "#             for t in range(L):\n",
        "#                 out,hidden=self.dec(inp,hidden)\n",
        "#                 outputs.append(out.unsqueeze(1))\n",
        "#                 inp = tgt[:,t] if torch.rand(1).item()<tf else out.argmax(1)\n",
        "#             return torch.cat(outputs,dim=1)\n",
        "#         # inference\n",
        "#         if not beam:\n",
        "#             seqs=[]\n",
        "#             for _ in range(self.mx):\n",
        "#                 out,hidden=self.dec(inp,hidden)\n",
        "#                 top1=out.argmax(1)\n",
        "#                 seqs.append(top1.unsqueeze(1))\n",
        "#                 inp=top1\n",
        "#                 if (top1==self.eos).all(): break\n",
        "#             return torch.cat(seqs,dim=1)\n",
        "#         # beam search\n",
        "#         all_out=[]\n",
        "#         for i in range(bs):\n",
        "#             beams=[([self.bos],0.0,hidden[:,i:i+1,:])]\n",
        "#             for _ in range(self.mx):\n",
        "#                 nb=[]\n",
        "#                 for seq,sc,hi in beams:\n",
        "#                     last=src.new_tensor([seq[-1]])\n",
        "#                     out,hn=self.dec(last,hi)\n",
        "#                     vals,inds=out.topk(k)\n",
        "#                     for j in range(k):\n",
        "#                         nb.append((seq+[inds[0,j].item()], sc+vals[0,j].item(), hn))\n",
        "#                 beams=sorted(nb,key=lambda x:x[1],reverse=True)[:k]\n",
        "#                 if all(s[-1]==self.eos for s,_,_ in beams): break\n",
        "#             best=beams[0][0][1:]  # drop BOS\n",
        "#             all_out.append(torch.tensor(best,device=src.device))\n",
        "#         # pad\n",
        "#         ml=max(o.size(0) for o in all_out)\n",
        "#         padded=[F.pad(o,(0,ml-o.size(0)),value=self.eos) for o in all_out]\n",
        "#         return torch.stack(padded,dim=0)\n",
        "\n",
        "# # ------------------------\n",
        "# # 5. Instantiate Model\n",
        "# # ------------------------\n",
        "# embed_dim, hid_dim = 300, 300\n",
        "# bos, eos = vocab['<bos>'], vocab['<eos>']\n",
        "\n",
        "# # choose encoder\n",
        "# if use_hier:\n",
        "#     encoder = HierEncoderRNN(vocab_size, embed_dim, hid_dim, hid_dim)\n",
        "# else:\n",
        "#     encoder = EncoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "# # load GloVe if requested\n",
        "# if use_glove:\n",
        "#     print(\"→ Loading GloVe embeddings...\")\n",
        "#     def load_glove(path):\n",
        "#         E = torch.randn(vocab_size, embed_dim)\n",
        "#         with open(path,'r',encoding='utf-8') as f:\n",
        "#             for l in f:\n",
        "#                 p=l.split()\n",
        "#                 w,vec=p[0],torch.tensor(list(map(float,p[1:])))\n",
        "#                 if w in vocab: E[vocab[w]] = vec\n",
        "#         return E\n",
        "#     embw = load_glove(config.glove_path if 'config' in globals() else '/content/glove.6B.300d.txt')\n",
        "#     encoder.load_embeddings(embw)\n",
        "\n",
        "# # choose decoder\n",
        "# decoder = Decoder2RNN(vocab_size, embed_dim, hid_dim) if use_decoder2 else DecoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "# model = Seq2seqRNN(encoder, decoder, bos, eos, mx=20).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# # ------------------------\n",
        "# # 6. Training & Eval\n",
        "# # ------------------------\n",
        "# opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "# crit=nn.NLLLoss(ignore_index=0)\n",
        "# scaler=torch.cuda.amp.GradScaler()\n",
        "\n",
        "# def train_epoch():\n",
        "#     model.train(); total=0\n",
        "#     for src,tgt in train_loader:\n",
        "#         src,tgt=src.to(model.dec.emb.weight.device),tgt.to(model.dec.emb.weight.device)\n",
        "#         opt.zero_grad()\n",
        "#         with torch.cuda.amp.autocast():\n",
        "#             out=model(src,tgt,tf=0.5,beam=False)\n",
        "#             loss=crit(out.view(-1,out.size(-1)),tgt.view(-1))\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(opt); scaler.update()\n",
        "#         total+=loss.item()\n",
        "#     return total/len(train_loader)\n",
        "\n",
        "# def eval_epoch():\n",
        "#     model.eval(); total=0\n",
        "#     with torch.no_grad():\n",
        "#         for src,tgt in val_loader:\n",
        "#             src,tgt=src.to(model.dec.emb.weight.device),tgt.to(model.dec.emb.weight.device)\n",
        "#             out=model(src,tgt,tf=0,beam=False)\n",
        "#             total+=crit(out.view(-1,out.size(-1)),tgt.view(-1)).item()\n",
        "#     return total/len(val_loader)\n",
        "\n",
        "# for ep in range(1,6):\n",
        "#     t0=time.time()\n",
        "#     tr=train_epoch(); va=eval_epoch()\n",
        "#     print(f\"Epoch {ep}: train {tr:.3f} | val {va:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "# # ------------------------\n",
        "# # 7. ROUGE Evaluation\n",
        "# # ------------------------\n",
        "# scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'],use_stemmer=True)\n",
        "# r1=r2=rL=cnt=0\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     for src,tgt in test_loader:\n",
        "#         src=src.to(model.dec.emb.weight.device)\n",
        "#         pred_seq = model(src, tgt=None, beam=use_beam, k=beam_width).squeeze(0).tolist()\n",
        "#         # cut at EOS\n",
        "#         if eos in pred_seq: pred_seq=pred_seq[:pred_seq.index(eos)]\n",
        "#         pred_txt = seq2text(pred_seq)\n",
        "#         tgt_seq = tgt.squeeze(0).tolist()\n",
        "#         tgt_seq=[i for i in tgt_seq if i not in (bos,eos,0)]\n",
        "#         tgt_txt=seq2text(tgt_seq)\n",
        "#         scr = scorer.score(tgt_txt,pred_txt)\n",
        "#         r1+=scr['rouge1'].fmeasure\n",
        "#         r2+=scr['rouge2'].fmeasure\n",
        "#         rL+=scr['rougeL'].fmeasure\n",
        "#         cnt+=1\n",
        "\n",
        "# print(f\"\\n→ ROUGE on test ({'beam' if use_beam else 'greedy'}):\")\n",
        "# print(f\"   ROUGE‑1: {r1/cnt:.4f}\")\n",
        "# print(f\"   ROUGE‑2: {r2/cnt:.4f}\")\n",
        "# print(f\"   ROUGE‑L: {rL/cnt:.4f}\")\n",
        "import os\n",
        "# Mitigate CUDA fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "# Set environment variable for CUDA memory fragmentation mitigation\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "!pip install rouge-score\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "# ------------------------\n",
        "# 1. Runtime Configuration\n",
        "# ------------------------\n",
        "use_glove     = input(\"Use GloVe embeddings? (y/n): \").strip().lower().startswith('y')\n",
        "glove_path = None\n",
        "if use_glove:\n",
        "    glove_path = input(\"→ Path to your GloVe file (e.g. /content/glove.6B.300d.txt): \").strip()\n",
        "    if not os.path.isfile(glove_path):\n",
        "        raise FileNotFoundError(f\"GloVe file not found at '{glove_path}'\")\n",
        "    # Infer embedding dimension from the first line\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        first = f.readline().split()\n",
        "        inferred_dim = len(first) - 1\n",
        "    print(f\"Detected GloVe embedding dimension: {inferred_dim}\")\n",
        "else:\n",
        "    inferred_dim = 300  # default if not using GloVe\n",
        "\n",
        "use_hier      = input(\"Use Hierarchical Encoder? (y/n): \").strip().lower().startswith('y')\n",
        "use_decoder2  = input(\"Use 2‑GRU Decoder? (y/n): \").strip().lower().startswith('y')\n",
        "use_beam      = input(\"Use Beam Search? (y/n): \").strip().lower().startswith('y')\n",
        "beam_width    = int(input(\"Beam width (e.g. 3): \").strip() or 3)\n",
        "\n",
        "print(f\"\\nConfig → GloVe: {use_glove}, Hier: {use_hier}, Decoder2: {use_decoder2}, Beam: {use_beam}, k={beam_width}\\n\")\n",
        "\n",
        "# ------------------------\n",
        "# 2. Load Data & Build Vocab\n",
        "# ------------------------\n",
        "train_df = pd.read_csv('/content/train_processed (1).csv')\n",
        "val_df   = pd.read_csv('/content/val_processed (1).csv')\n",
        "test_df  = pd.read_csv('/content/test_processed (1).csv')\n",
        "\n",
        "def tokenize(txt): return txt.split()\n",
        "\n",
        "min_freq = 0.001 * len(train_df)\n",
        "ctr = Counter()\n",
        "for col in ['text','title']:\n",
        "    for s in train_df[col]:\n",
        "        ctr.update(tokenize(s))\n",
        "\n",
        "vocab = {'<pad>':0,'<bos>':1,'<eos>':2,'<unk>':3}\n",
        "for tok,f in ctr.items():\n",
        "    if f >= min_freq:\n",
        "        vocab[tok] = len(vocab)\n",
        "inv_vocab = {i:t for t,i in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "def text2seq(s):\n",
        "    return [vocab.get(t, vocab['<unk>']) for t in tokenize(s)]\n",
        "def seq2text(seq):\n",
        "    return \" \".join(inv_vocab.get(i,'<unk>') for i in seq)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Dataset & Dataloader\n",
        "# ------------------------\n",
        "class TitleDS(Dataset):\n",
        "    def __init__(self,df): self.df=df\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self,i):\n",
        "        row=self.df.iloc[i]\n",
        "        inp=text2seq(row['text'])\n",
        "        tgt=[vocab['<bos>']] + text2seq(row['title']) + [vocab['<eos>']]\n",
        "        return torch.tensor(inp), torch.tensor(tgt)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ins,ts = zip(*batch)\n",
        "    Mi, Mt = max(len(x) for x in ins), max(len(x) for x in ts)\n",
        "    ins = torch.stack([F.pad(x,(0,Mi-len(x)),value=vocab['<pad>']) for x in ins])\n",
        "    ts  = torch.stack([F.pad(x,(0,Mt-len(x)),value=vocab['<pad>']) for x in ts])\n",
        "    return ins, ts\n",
        "\n",
        "train_loader = DataLoader(TitleDS(train_df), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(TitleDS(val_df),   batch_size=8, shuffle=False,collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(TitleDS(test_df),  batch_size=1, shuffle=False,collate_fn=collate_fn)\n",
        "\n",
        "# ------------------------\n",
        "# 4. Model Components\n",
        "# ------------------------\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        return self.gru(self.emb(x))\n",
        "    def load_embeddings(self,weights):\n",
        "        self.emb.weight.data.copy_(weights)\n",
        "\n",
        "class HierEncoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,wh,sh):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.wgru=nn.GRU(ed,wh,batch_first=True)\n",
        "        self.sgru=nn.GRU(wh,sh,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        wo,wh = self.wgru(self.emb(x))\n",
        "        se = wo.mean(dim=1,keepdim=True)\n",
        "        return self.sgru(se)\n",
        "    def load_embeddings(self,weights):\n",
        "        self.emb.weight.data.copy_(weights)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "        self.fc = nn.Linear(hd,vs)\n",
        "    def forward(self,tk,h):\n",
        "        o,h = self.gru(self.emb(tk.unsqueeze(1)),h)\n",
        "        return F.log_softmax(self.fc(o.squeeze(1)),dim=1), h\n",
        "\n",
        "class Decoder2RNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.g1 = nn.GRU(ed,hd,batch_first=True)\n",
        "        self.g2 = nn.GRU(hd,hd,batch_first=True)\n",
        "        self.fc = nn.Linear(hd,vs)\n",
        "    def forward(self,tk,h):\n",
        "        o1,h1 = self.g1(self.emb(tk.unsqueeze(1)),h)\n",
        "        o2,h2 = self.g2(o1,h1)\n",
        "        return F.log_softmax(self.fc(o2.squeeze(1)),dim=1), h2\n",
        "\n",
        "class Seq2seqRNN(nn.Module):\n",
        "    def __init__(self,enc,dec,bos,eos,mx=20):\n",
        "        super().__init__()\n",
        "        self.enc, self.dec = enc, dec\n",
        "        self.bos, self.eos, self.mx = bos, eos, mx\n",
        "\n",
        "    def forward(self,src,tgt=None,tf=0.5,beam=False,k=3):\n",
        "        bs = src.size(0)\n",
        "        eo,hidden = self.enc(src)\n",
        "        inp = src.new_full((bs,), self.bos)\n",
        "        outputs=[]\n",
        "        if tgt is not None:\n",
        "            for t in range(tgt.size(1)):\n",
        "                out,hidden = self.dec(inp,hidden)\n",
        "                outputs.append(out.unsqueeze(1))\n",
        "                inp = tgt[:,t] if torch.rand(1).item()<tf else out.argmax(1)\n",
        "            return torch.cat(outputs,dim=1)\n",
        "        # inference\n",
        "        if not beam:\n",
        "            seqs=[]\n",
        "            for _ in range(self.mx):\n",
        "                out,hidden = self.dec(inp,hidden)\n",
        "                top1 = out.argmax(1)\n",
        "                seqs.append(top1.unsqueeze(1))\n",
        "                inp = top1\n",
        "                if (top1==self.eos).all(): break\n",
        "            return torch.cat(seqs,dim=1)\n",
        "        # beam search\n",
        "        all_out=[]\n",
        "        for i in range(bs):\n",
        "            beams=[([self.bos],0.0,hidden[:,i:i+1,:])]\n",
        "            for _ in range(self.mx):\n",
        "                nb=[]\n",
        "                for seq,sc,hi in beams:\n",
        "                    last = src.new_tensor([seq[-1]])\n",
        "                    out,hn = self.dec(last,hi)\n",
        "                    vals,inds = out.topk(k)\n",
        "                    for j in range(k):\n",
        "                        nb.append((seq+[inds[0,j].item()], sc+vals[0,j].item(), hn))\n",
        "                beams = sorted(nb, key=lambda x:x[1], reverse=True)[:k]\n",
        "                if all(s[-1]==self.eos for s,_,_ in beams): break\n",
        "            best = beams[0][0][1:]\n",
        "            all_out.append(torch.tensor(best,device=src.device))\n",
        "        ml = max(o.size(0) for o in all_out)\n",
        "        return torch.stack([F.pad(o,(0,ml-o.size(0)),value=self.eos) for o in all_out],dim=0)\n",
        "\n",
        "# ------------------------\n",
        "# 5. Instantiate Model\n",
        "# ------------------------\n",
        "embed_dim = inferred_dim\n",
        "hid_dim   = 300\n",
        "bos, eos  = vocab['<bos>'], vocab['<eos>']\n",
        "\n",
        "# Encoder choice\n",
        "encoder = HierEncoderRNN(vocab_size, embed_dim, hid_dim, hid_dim) if use_hier \\\n",
        "          else EncoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "# Load GloVe if requested\n",
        "if use_glove:\n",
        "    print(\"→ Loading GloVe embeddings from\", glove_path)\n",
        "    def load_glove(path, vs, ed):\n",
        "        E = torch.randn(vs, ed)\n",
        "        with open(path,'r',encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.split()\n",
        "                w, vec = parts[0], torch.tensor(list(map(float,parts[1:])))\n",
        "                idx = vocab.get(w)\n",
        "                if idx is not None:\n",
        "                    E[idx] = vec\n",
        "        return E\n",
        "    encoder.load_embeddings(load_glove(glove_path, vocab_size, embed_dim))\n",
        "\n",
        "# Decoder choice\n",
        "decoder = Decoder2RNN(vocab_size, embed_dim, hid_dim) if use_decoder2 \\\n",
        "          else DecoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model  = Seq2seqRNN(encoder, decoder, bos, eos, mx=20).to(device)\n",
        "\n",
        "# ------------------------\n",
        "# 6. Training & Eval\n",
        "# ------------------------\n",
        "opt    = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "crit   = nn.NLLLoss(ignore_index=vocab['<pad>'])\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def train_epoch():\n",
        "    model.train(); total=0\n",
        "    for src,tgt in train_loader:\n",
        "        src,tgt = src.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(src,tgt,tf=0.5,beam=False)\n",
        "            loss = crit(out.view(-1,out.size(-1)), tgt.view(-1))\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt); scaler.update()\n",
        "        total += loss.item()\n",
        "    return total/len(train_loader)\n",
        "\n",
        "def eval_epoch():\n",
        "    model.eval(); total=0\n",
        "    with torch.no_grad():\n",
        "        for src,tgt in val_loader:\n",
        "            src,tgt = src.to(device), tgt.to(device)\n",
        "            out = model(src,tgt,tf=0,beam=False)\n",
        "            total += crit(out.view(-1,out.size(-1)), tgt.view(-1)).item()\n",
        "    return total/len(val_loader)\n",
        "\n",
        "for ep in range(1,6):\n",
        "    t0 = time.time()\n",
        "    tr = train_epoch(); va = eval_epoch()\n",
        "    print(f\"Epoch {ep}: train {tr:.3f} | val {va:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "# ------------------------\n",
        "# 7. ROUGE Evaluation\n",
        "# ------------------------\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
        "r1=r2=rL=cnt=0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for src,tgt in test_loader:\n",
        "        src = src.to(device)\n",
        "        pred_seq = model(src, tgt=None, beam=use_beam, k=beam_width).squeeze(0).tolist()\n",
        "        if eos in pred_seq: pred_seq = pred_seq[:pred_seq.index(eos)]\n",
        "        pred_txt = seq2text(pred_seq)\n",
        "        tgt_seq = [i for i in tgt.squeeze(0).tolist() if i not in (bos,eos,vocab['<pad>'])]\n",
        "        tgt_txt  = seq2text(tgt_seq)\n",
        "        scr = scorer.score(tgt_txt, pred_txt)\n",
        "        r1 += scr['rouge1'].fmeasure\n",
        "        r2 += scr['rouge2'].fmeasure\n",
        "        rL += scr['rougeL'].fmeasure\n",
        "        cnt += 1\n",
        "\n",
        "print(f\"\\n→ ROUGE on test ({'beam' if use_beam else 'greedy'}):\")\n",
        "print(f\"   ROUGE‑1: {r1/cnt:.4f}\")\n",
        "print(f\"   ROUGE‑2: {r2/cnt:.4f}\")\n",
        "print(f\"   ROUGE‑L: {rL/cnt:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnjN0hbQDvLk",
        "outputId": "18d4913c-1209-49d7-807b-118e31bc2d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GloVe embeddings? (y/n): n\n",
            "Use Hierarchical Encoder? (y/n): n\n",
            "Use 2‑GRU Decoder? (y/n): y\n",
            "Use Beam Search? (y/n): n\n",
            "Beam width (e.g. 3): 3\n",
            "\n",
            "Config → GloVe: False, Hier: False, Decoder2: True, Beam: False, k=3\n",
            "\n",
            "Vocab size: 54473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60874a53fac4>:527: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-2-60874a53fac4>:534: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train 1.673 | val 1.355 | 274.0s\n",
            "Epoch 2: train 1.100 | val 1.085 | 270.3s\n",
            "Epoch 3: train 0.818 | val 0.931 | 270.2s\n",
            "Epoch 4: train 0.604 | val 0.886 | 271.5s\n",
            "Epoch 5: train 0.463 | val 0.847 | 271.8s\n",
            "\n",
            "→ ROUGE on test (greedy):\n",
            "   ROUGE‑1: 0.6688\n",
            "   ROUGE‑2: 0.3833\n",
            "   ROUGE‑L: 0.6630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# # Mitigate CUDA fragmentation\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import time\n",
        "# from collections import Counter\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import nltk\n",
        "# import pandas as pd\n",
        "# import re\n",
        "# import nltk\n",
        "# import string\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import time\n",
        "# from collections import Counter\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# !pip install rouge-score\n",
        "# nltk.download('punkt')\n",
        "# from rouge_score import rouge_scorer\n",
        "\n",
        "# # ------------------------\n",
        "# # 1. Runtime Configuration\n",
        "# # ------------------------\n",
        "# use_glove     = input(\"Use GloVe embeddings? (y/n): \").strip().lower().startswith('y')\n",
        "# use_hier      = input(\"Use Hierarchical Encoder? (y/n): \").strip().lower().startswith('y')\n",
        "# use_decoder2  = input(\"Use 2‑GRU Decoder? (y/n): \").strip().lower().startswith('y')\n",
        "# use_beam      = input(\"Use Beam Search? (y/n): \").strip().lower().startswith('y')\n",
        "# beam_width    = int(input(\"Beam width (e.g. 3): \").strip() or 3)\n",
        "\n",
        "# print(f\"\\nConfig → GloVe: {use_glove}, Hier: {use_hier}, Decoder2: {use_decoder2}, Beam: {use_beam}, k={beam_width}\\n\")\n",
        "\n",
        "# # ------------------------\n",
        "# # 2. Load Data & Build Vocab\n",
        "# # ------------------------\n",
        "# train_df = pd.read_csv('/content/train_processed.csv')\n",
        "# val_df   = pd.read_csv('/content/val_processed.csv')\n",
        "# test_df  = pd.read_csv('/content/test_processed.csv')\n",
        "\n",
        "# def tokenize(txt): return txt.split()\n",
        "\n",
        "# # Build freq counter\n",
        "# min_freq = 0.001 * len(train_df)\n",
        "# ctr = Counter()\n",
        "# for col in ['text','title']:\n",
        "#     for s in train_df[col]:\n",
        "#         ctr.update(tokenize(s))\n",
        "\n",
        "# # Build vocab\n",
        "# vocab = {'<pad>':0,'<bos>':1,'<eos>':2,'<unk>':3}\n",
        "# for tok,f in ctr.items():\n",
        "#     if f>=min_freq:\n",
        "#         vocab[tok] = len(vocab)\n",
        "# inv_vocab = {i:t for t,i in vocab.items()}\n",
        "# vocab_size = len(vocab)\n",
        "# print(\"Vocab size:\",vocab_size)\n",
        "\n",
        "# def text2seq(s):\n",
        "#     return [vocab.get(t,4) for t in tokenize(s)]\n",
        "# def seq2text(seq):\n",
        "#     return \" \".join(inv_vocab.get(i,'<unk>') for i in seq)\n",
        "\n",
        "# # ------------------------\n",
        "# # 3. Dataset & Dataloader\n",
        "# # ------------------------\n",
        "# class TitleDS(Dataset):\n",
        "#     def __init__(self,df):\n",
        "#         self.df=df\n",
        "#     def __len__(self): return len(self.df)\n",
        "#     def __getitem__(self,i):\n",
        "#         row=self.df.iloc[i]\n",
        "#         inp=text2seq(row['text'])\n",
        "#         tgt=[vocab['<bos>']]+text2seq(row['title'])+[vocab['<eos>']]\n",
        "#         return torch.tensor(inp), torch.tensor(tgt)\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     ins,ts = zip(*batch)\n",
        "#     li=[len(x) for x in ins]; lt=[len(x) for x in ts]\n",
        "#     Mi, Mt = max(li), max(lt)\n",
        "#     ins = torch.stack([F.pad(x,(0,Mi-len(x)),value=0) for x in ins])\n",
        "#     ts  = torch.stack([F.pad(x,(0,Mt-len(x)),value=0) for x in ts])\n",
        "#     return ins, ts\n",
        "\n",
        "# train_loader = DataLoader(TitleDS(train_df), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "# val_loader   = DataLoader(TitleDS(val_df),   batch_size=8, shuffle=False,collate_fn=collate_fn)\n",
        "# test_loader  = DataLoader(TitleDS(test_df),  batch_size=1, shuffle=False,collate_fn=collate_fn)\n",
        "\n",
        "# # ------------------------\n",
        "# # 4. Model Components\n",
        "# # ------------------------\n",
        "# class EncoderRNN(nn.Module):\n",
        "#     def __init__(self,vs,ed,hd,drop=0.2):\n",
        "#         super().__init__()\n",
        "#         self.emb=nn.Embedding(vs,ed)\n",
        "#         self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "#     def forward(self,x):\n",
        "#         e=self.emb(x)\n",
        "#         o,h=self.gru(e)\n",
        "#         return o,h\n",
        "#     def load_embeddings(self,weights):\n",
        "#         self.emb.weight.data.copy_(weights)\n",
        "\n",
        "# class HierEncoderRNN(nn.Module):\n",
        "#     def __init__(self,vs,ed,wh,sh,drop=0.2):\n",
        "#         super().__init__()\n",
        "#         self.emb=nn.Embedding(vs,ed)\n",
        "#         self.wgru=nn.GRU(ed,wh,batch_first=True)\n",
        "#         self.sgru=nn.GRU(wh,sh,batch_first=True)\n",
        "#     def forward(self,x):\n",
        "#         e=self.emb(x)\n",
        "#         wo,wh=self.wgru(e)\n",
        "#         # average over time → sentence embedding\n",
        "#         se=wo.mean(dim=1,keepdim=True)\n",
        "#         so,sh=self.sgru(se)\n",
        "#         return so,sh\n",
        "#     def load_embeddings(self,weights):\n",
        "#         self.emb.weight.data.copy_(weights)\n",
        "\n",
        "# class DecoderRNN(nn.Module):\n",
        "#     def __init__(self,vs,ed,hd,drop=0.2):\n",
        "#         super().__init__()\n",
        "#         self.emb=nn.Embedding(vs,ed)\n",
        "#         self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "#         self.fc=nn.Linear(hd,vs)\n",
        "#     def forward(self,token,hidden):\n",
        "#         e=self.emb(token.unsqueeze(1))\n",
        "#         o,h=self.gru(e,hidden)\n",
        "#         return F.log_softmax(self.fc(o.squeeze(1)),dim=1), h\n",
        "\n",
        "# class Decoder2RNN(nn.Module):\n",
        "#     def __init__(self,vs,ed,hd,drop=0.2):\n",
        "#         super().__init__()\n",
        "#         self.emb=nn.Embedding(vs,ed)\n",
        "#         self.g1=nn.GRU(ed,hd,batch_first=True)\n",
        "#         self.g2=nn.GRU(hd,hd,batch_first=True)\n",
        "#         self.fc=nn.Linear(hd,vs)\n",
        "#     def forward(self,token,hidden):\n",
        "#         e=self.emb(token.unsqueeze(1))\n",
        "#         o1,h1=self.g1(e,hidden)\n",
        "#         o2,h2=self.g2(o1,h1)\n",
        "#         return F.log_softmax(self.fc(o2.squeeze(1)),dim=1), h2\n",
        "\n",
        "# class Seq2seqRNN(nn.Module):\n",
        "#     def __init__(self,enc,dec,bos,eos,mx=20):\n",
        "#         super().__init__()\n",
        "#         self.enc, self.dec = enc, dec\n",
        "#         self.bos, self.eos, self.mx = bos,eos,mx\n",
        "\n",
        "#     def forward(self,src,tgt=None,tf=0.5,beam=False,k=3):\n",
        "#         bs=src.size(0)\n",
        "#         eo,hidden = self.enc(src)\n",
        "#         inp = src.new_full((bs,),self.bos)\n",
        "#         outputs=[]\n",
        "#         if tgt is not None:\n",
        "#             L=tgt.size(1)\n",
        "#             for t in range(L):\n",
        "#                 out,hidden=self.dec(inp,hidden)\n",
        "#                 outputs.append(out.unsqueeze(1))\n",
        "#                 inp = tgt[:,t] if torch.rand(1).item()<tf else out.argmax(1)\n",
        "#             return torch.cat(outputs,dim=1)\n",
        "#         # inference\n",
        "#         if not beam:\n",
        "#             seqs=[]\n",
        "#             for _ in range(self.mx):\n",
        "#                 out,hidden=self.dec(inp,hidden)\n",
        "#                 top1=out.argmax(1)\n",
        "#                 seqs.append(top1.unsqueeze(1))\n",
        "#                 inp=top1\n",
        "#                 if (top1==self.eos).all(): break\n",
        "#             return torch.cat(seqs,dim=1)\n",
        "#         # beam search\n",
        "#         all_out=[]\n",
        "#         for i in range(bs):\n",
        "#             beams=[([self.bos],0.0,hidden[:,i:i+1,:])]\n",
        "#             for _ in range(self.mx):\n",
        "#                 nb=[]\n",
        "#                 for seq,sc,hi in beams:\n",
        "#                     last=src.new_tensor([seq[-1]])\n",
        "#                     out,hn=self.dec(last,hi)\n",
        "#                     vals,inds=out.topk(k)\n",
        "#                     for j in range(k):\n",
        "#                         nb.append((seq+[inds[0,j].item()], sc+vals[0,j].item(), hn))\n",
        "#                 beams=sorted(nb,key=lambda x:x[1],reverse=True)[:k]\n",
        "#                 if all(s[-1]==self.eos for s,_,_ in beams): break\n",
        "#             best=beams[0][0][1:]  # drop BOS\n",
        "#             all_out.append(torch.tensor(best,device=src.device))\n",
        "#         # pad\n",
        "#         ml=max(o.size(0) for o in all_out)\n",
        "#         padded=[F.pad(o,(0,ml-o.size(0)),value=self.eos) for o in all_out]\n",
        "#         return torch.stack(padded,dim=0)\n",
        "\n",
        "# # ------------------------\n",
        "# # 5. Instantiate Model\n",
        "# # ------------------------\n",
        "# embed_dim, hid_dim = 300, 300\n",
        "# bos, eos = vocab['<bos>'], vocab['<eos>']\n",
        "\n",
        "# # choose encoder\n",
        "# if use_hier:\n",
        "#     encoder = HierEncoderRNN(vocab_size, embed_dim, hid_dim, hid_dim)\n",
        "# else:\n",
        "#     encoder = EncoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "# # load GloVe if requested\n",
        "# if use_glove:\n",
        "#     print(\"→ Loading GloVe embeddings...\")\n",
        "#     def load_glove(path):\n",
        "#         E = torch.randn(vocab_size, embed_dim)\n",
        "#         with open(path,'r',encoding='utf-8') as f:\n",
        "#             for l in f:\n",
        "#                 p=l.split()\n",
        "#                 w,vec=p[0],torch.tensor(list(map(float,p[1:])))\n",
        "#                 if w in vocab: E[vocab[w]] = vec\n",
        "#         return E\n",
        "#     embw = load_glove(config.glove_path if 'config' in globals() else '/content/glove.6B.300d.txt')\n",
        "#     encoder.load_embeddings(embw)\n",
        "\n",
        "# # choose decoder\n",
        "# decoder = Decoder2RNN(vocab_size, embed_dim, hid_dim) if use_decoder2 else DecoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "# model = Seq2seqRNN(encoder, decoder, bos, eos, mx=20).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# # ------------------------\n",
        "# # 6. Training & Eval\n",
        "# # ------------------------\n",
        "# opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "# crit=nn.NLLLoss(ignore_index=0)\n",
        "# scaler=torch.cuda.amp.GradScaler()\n",
        "\n",
        "# def train_epoch():\n",
        "#     model.train(); total=0\n",
        "#     for src,tgt in train_loader:\n",
        "#         src,tgt=src.to(model.dec.emb.weight.device),tgt.to(model.dec.emb.weight.device)\n",
        "#         opt.zero_grad()\n",
        "#         with torch.cuda.amp.autocast():\n",
        "#             out=model(src,tgt,tf=0.5,beam=False)\n",
        "#             loss=crit(out.view(-1,out.size(-1)),tgt.view(-1))\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(opt); scaler.update()\n",
        "#         total+=loss.item()\n",
        "#     return total/len(train_loader)\n",
        "\n",
        "# def eval_epoch():\n",
        "#     model.eval(); total=0\n",
        "#     with torch.no_grad():\n",
        "#         for src,tgt in val_loader:\n",
        "#             src,tgt=src.to(model.dec.emb.weight.device),tgt.to(model.dec.emb.weight.device)\n",
        "#             out=model(src,tgt,tf=0,beam=False)\n",
        "#             total+=crit(out.view(-1,out.size(-1)),tgt.view(-1)).item()\n",
        "#     return total/len(val_loader)\n",
        "\n",
        "# for ep in range(1,6):\n",
        "#     t0=time.time()\n",
        "#     tr=train_epoch(); va=eval_epoch()\n",
        "#     print(f\"Epoch {ep}: train {tr:.3f} | val {va:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "# # ------------------------\n",
        "# # 7. ROUGE Evaluation\n",
        "# # ------------------------\n",
        "# scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'],use_stemmer=True)\n",
        "# r1=r2=rL=cnt=0\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     for src,tgt in test_loader:\n",
        "#         src=src.to(model.dec.emb.weight.device)\n",
        "#         pred_seq = model(src, tgt=None, beam=use_beam, k=beam_width).squeeze(0).tolist()\n",
        "#         # cut at EOS\n",
        "#         if eos in pred_seq: pred_seq=pred_seq[:pred_seq.index(eos)]\n",
        "#         pred_txt = seq2text(pred_seq)\n",
        "#         tgt_seq = tgt.squeeze(0).tolist()\n",
        "#         tgt_seq=[i for i in tgt_seq if i not in (bos,eos,0)]\n",
        "#         tgt_txt=seq2text(tgt_seq)\n",
        "#         scr = scorer.score(tgt_txt,pred_txt)\n",
        "#         r1+=scr['rouge1'].fmeasure\n",
        "#         r2+=scr['rouge2'].fmeasure\n",
        "#         rL+=scr['rougeL'].fmeasure\n",
        "#         cnt+=1\n",
        "\n",
        "# print(f\"\\n→ ROUGE on test ({'beam' if use_beam else 'greedy'}):\")\n",
        "# print(f\"   ROUGE‑1: {r1/cnt:.4f}\")\n",
        "# print(f\"   ROUGE‑2: {r2/cnt:.4f}\")\n",
        "# print(f\"   ROUGE‑L: {rL/cnt:.4f}\")\n",
        "import os\n",
        "# Mitigate CUDA fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "# Set environment variable for CUDA memory fragmentation mitigation\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "!pip install rouge-score\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "# ------------------------\n",
        "# 1. Runtime Configuration\n",
        "# ------------------------\n",
        "use_glove     = input(\"Use GloVe embeddings? (y/n): \").strip().lower().startswith('y')\n",
        "glove_path = None\n",
        "if use_glove:\n",
        "    glove_path = input(\"→ Path to your GloVe file (e.g. /content/glove.6B.300d.txt): \").strip()\n",
        "    if not os.path.isfile(glove_path):\n",
        "        raise FileNotFoundError(f\"GloVe file not found at '{glove_path}'\")\n",
        "    # Infer embedding dimension from the first line\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        first = f.readline().split()\n",
        "        inferred_dim = len(first) - 1\n",
        "    print(f\"Detected GloVe embedding dimension: {inferred_dim}\")\n",
        "else:\n",
        "    inferred_dim = 300  # default if not using GloVe\n",
        "\n",
        "use_hier      = input(\"Use Hierarchical Encoder? (y/n): \").strip().lower().startswith('y')\n",
        "use_decoder2  = input(\"Use 2‑GRU Decoder? (y/n): \").strip().lower().startswith('y')\n",
        "use_beam      = input(\"Use Beam Search? (y/n): \").strip().lower().startswith('y')\n",
        "beam_width    = int(input(\"Beam width (e.g. 3): \").strip() or 3)\n",
        "\n",
        "print(f\"\\nConfig → GloVe: {use_glove}, Hier: {use_hier}, Decoder2: {use_decoder2}, Beam: {use_beam}, k={beam_width}\\n\")\n",
        "\n",
        "# ------------------------\n",
        "# 2. Load Data & Build Vocab\n",
        "# ------------------------\n",
        "train_df = pd.read_csv('/content/train_processed (1).csv')\n",
        "val_df   = pd.read_csv('/content/val_processed (1).csv')\n",
        "test_df  = pd.read_csv('/content/test_processed (1).csv')\n",
        "\n",
        "def tokenize(txt): return txt.split()\n",
        "\n",
        "min_freq = 0.001 * len(train_df)\n",
        "ctr = Counter()\n",
        "for col in ['text','title']:\n",
        "    for s in train_df[col]:\n",
        "        ctr.update(tokenize(s))\n",
        "\n",
        "vocab = {'<pad>':0,'<bos>':1,'<eos>':2,'<unk>':3}\n",
        "for tok,f in ctr.items():\n",
        "    if f >= min_freq:\n",
        "        vocab[tok] = len(vocab)\n",
        "inv_vocab = {i:t for t,i in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "def text2seq(s):\n",
        "    return [vocab.get(t, vocab['<unk>']) for t in tokenize(s)]\n",
        "def seq2text(seq):\n",
        "    return \" \".join(inv_vocab.get(i,'<unk>') for i in seq)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Dataset & Dataloader\n",
        "# ------------------------\n",
        "class TitleDS(Dataset):\n",
        "    def __init__(self,df): self.df=df\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self,i):\n",
        "        row=self.df.iloc[i]\n",
        "        inp=text2seq(row['text'])\n",
        "        tgt=[vocab['<bos>']] + text2seq(row['title']) + [vocab['<eos>']]\n",
        "        return torch.tensor(inp), torch.tensor(tgt)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ins,ts = zip(*batch)\n",
        "    Mi, Mt = max(len(x) for x in ins), max(len(x) for x in ts)\n",
        "    ins = torch.stack([F.pad(x,(0,Mi-len(x)),value=vocab['<pad>']) for x in ins])\n",
        "    ts  = torch.stack([F.pad(x,(0,Mt-len(x)),value=vocab['<pad>']) for x in ts])\n",
        "    return ins, ts\n",
        "\n",
        "train_loader = DataLoader(TitleDS(train_df), batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(TitleDS(val_df),   batch_size=8, shuffle=False,collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(TitleDS(test_df),  batch_size=1, shuffle=False,collate_fn=collate_fn)\n",
        "\n",
        "# ------------------------\n",
        "# 4. Model Components\n",
        "# ------------------------\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        return self.gru(self.emb(x))\n",
        "    def load_embeddings(self,weights):\n",
        "        self.emb.weight.data.copy_(weights)\n",
        "\n",
        "class HierEncoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,wh,sh):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.wgru=nn.GRU(ed,wh,batch_first=True)\n",
        "        self.sgru=nn.GRU(wh,sh,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        wo,wh = self.wgru(self.emb(x))\n",
        "        se = wo.mean(dim=1,keepdim=True)\n",
        "        return self.sgru(se)\n",
        "    def load_embeddings(self,weights):\n",
        "        self.emb.weight.data.copy_(weights)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.gru=nn.GRU(ed,hd,batch_first=True)\n",
        "        self.fc = nn.Linear(hd,vs)\n",
        "    def forward(self,tk,h):\n",
        "        o,h = self.gru(self.emb(tk.unsqueeze(1)),h)\n",
        "        return F.log_softmax(self.fc(o.squeeze(1)),dim=1), h\n",
        "\n",
        "class Decoder2RNN(nn.Module):\n",
        "    def __init__(self,vs,ed,hd):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vs,ed)\n",
        "        self.g1 = nn.GRU(ed,hd,batch_first=True)\n",
        "        self.g2 = nn.GRU(hd,hd,batch_first=True)\n",
        "        self.fc = nn.Linear(hd,vs)\n",
        "    def forward(self,tk,h):\n",
        "        o1,h1 = self.g1(self.emb(tk.unsqueeze(1)),h)\n",
        "        o2,h2 = self.g2(o1,h1)\n",
        "        return F.log_softmax(self.fc(o2.squeeze(1)),dim=1), h2\n",
        "\n",
        "class Seq2seqRNN(nn.Module):\n",
        "    def __init__(self,enc,dec,bos,eos,mx=20):\n",
        "        super().__init__()\n",
        "        self.enc, self.dec = enc, dec\n",
        "        self.bos, self.eos, self.mx = bos, eos, mx\n",
        "\n",
        "    def forward(self,src,tgt=None,tf=0.5,beam=False,k=3):\n",
        "        bs = src.size(0)\n",
        "        eo,hidden = self.enc(src)\n",
        "        inp = src.new_full((bs,), self.bos)\n",
        "        outputs=[]\n",
        "        if tgt is not None:\n",
        "            for t in range(tgt.size(1)):\n",
        "                out,hidden = self.dec(inp,hidden)\n",
        "                outputs.append(out.unsqueeze(1))\n",
        "                inp = tgt[:,t] if torch.rand(1).item()<tf else out.argmax(1)\n",
        "            return torch.cat(outputs,dim=1)\n",
        "        # inference\n",
        "        if not beam:\n",
        "            seqs=[]\n",
        "            for _ in range(self.mx):\n",
        "                out,hidden = self.dec(inp,hidden)\n",
        "                top1 = out.argmax(1)\n",
        "                seqs.append(top1.unsqueeze(1))\n",
        "                inp = top1\n",
        "                if (top1==self.eos).all(): break\n",
        "            return torch.cat(seqs,dim=1)\n",
        "        # beam search\n",
        "        all_out=[]\n",
        "        for i in range(bs):\n",
        "            beams=[([self.bos],0.0,hidden[:,i:i+1,:])]\n",
        "            for _ in range(self.mx):\n",
        "                nb=[]\n",
        "                for seq,sc,hi in beams:\n",
        "                    last = src.new_tensor([seq[-1]])\n",
        "                    out,hn = self.dec(last,hi)\n",
        "                    vals,inds = out.topk(k)\n",
        "                    for j in range(k):\n",
        "                        nb.append((seq+[inds[0,j].item()], sc+vals[0,j].item(), hn))\n",
        "                beams = sorted(nb, key=lambda x:x[1], reverse=True)[:k]\n",
        "                if all(s[-1]==self.eos for s,_,_ in beams): break\n",
        "            best = beams[0][0][1:]\n",
        "            all_out.append(torch.tensor(best,device=src.device))\n",
        "        ml = max(o.size(0) for o in all_out)\n",
        "        return torch.stack([F.pad(o,(0,ml-o.size(0)),value=self.eos) for o in all_out],dim=0)\n",
        "\n",
        "# ------------------------\n",
        "# 5. Instantiate Model\n",
        "# ------------------------\n",
        "embed_dim = inferred_dim\n",
        "hid_dim   = 300\n",
        "bos, eos  = vocab['<bos>'], vocab['<eos>']\n",
        "\n",
        "# Encoder choice\n",
        "encoder = HierEncoderRNN(vocab_size, embed_dim, hid_dim, hid_dim) if use_hier \\\n",
        "          else EncoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "# Load GloVe if requested\n",
        "if use_glove:\n",
        "    print(\"→ Loading GloVe embeddings from\", glove_path)\n",
        "    def load_glove(path, vs, ed):\n",
        "        E = torch.randn(vs, ed)\n",
        "        with open(path,'r',encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.split()\n",
        "                w, vec = parts[0], torch.tensor(list(map(float,parts[1:])))\n",
        "                idx = vocab.get(w)\n",
        "                if idx is not None:\n",
        "                    E[idx] = vec\n",
        "        return E\n",
        "    encoder.load_embeddings(load_glove(glove_path, vocab_size, embed_dim))\n",
        "\n",
        "# Decoder choice\n",
        "decoder = Decoder2RNN(vocab_size, embed_dim, hid_dim) if use_decoder2 \\\n",
        "          else DecoderRNN(vocab_size, embed_dim, hid_dim)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model  = Seq2seqRNN(encoder, decoder, bos, eos, mx=20).to(device)\n",
        "\n",
        "# ------------------------\n",
        "# 6. Training & Eval\n",
        "# ------------------------\n",
        "opt    = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "crit   = nn.NLLLoss(ignore_index=vocab['<pad>'])\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def train_epoch():\n",
        "    model.train(); total=0\n",
        "    for src,tgt in train_loader:\n",
        "        src,tgt = src.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(src,tgt,tf=0.5,beam=False)\n",
        "            loss = crit(out.view(-1,out.size(-1)), tgt.view(-1))\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt); scaler.update()\n",
        "        total += loss.item()\n",
        "    return total/len(train_loader)\n",
        "\n",
        "def eval_epoch():\n",
        "    model.eval(); total=0\n",
        "    with torch.no_grad():\n",
        "        for src,tgt in val_loader:\n",
        "            src,tgt = src.to(device), tgt.to(device)\n",
        "            out = model(src,tgt,tf=0,beam=False)\n",
        "            total += crit(out.view(-1,out.size(-1)), tgt.view(-1)).item()\n",
        "    return total/len(val_loader)\n",
        "\n",
        "for ep in range(1,6):\n",
        "    t0 = time.time()\n",
        "    tr = train_epoch(); va = eval_epoch()\n",
        "    print(f\"Epoch {ep}: train {tr:.3f} | val {va:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "# ------------------------\n",
        "# 7. ROUGE Evaluation\n",
        "# ------------------------\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
        "r1=r2=rL=cnt=0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for src,tgt in test_loader:\n",
        "        src = src.to(device)\n",
        "        pred_seq = model(src, tgt=None, beam=use_beam, k=beam_width).squeeze(0).tolist()\n",
        "        if eos in pred_seq: pred_seq = pred_seq[:pred_seq.index(eos)]\n",
        "        pred_txt = seq2text(pred_seq)\n",
        "        tgt_seq = [i for i in tgt.squeeze(0).tolist() if i not in (bos,eos,vocab['<pad>'])]\n",
        "        tgt_txt  = seq2text(tgt_seq)\n",
        "        scr = scorer.score(tgt_txt, pred_txt)\n",
        "        r1 += scr['rouge1'].fmeasure\n",
        "        r2 += scr['rouge2'].fmeasure\n",
        "        rL += scr['rougeL'].fmeasure\n",
        "        cnt += 1\n",
        "\n",
        "print(f\"\\n→ ROUGE on test ({'beam' if use_beam else 'greedy'}):\")\n",
        "print(f\"   ROUGE‑1: {r1/cnt:.4f}\")\n",
        "print(f\"   ROUGE‑2: {r2/cnt:.4f}\")\n",
        "print(f\"   ROUGE‑L: {rL/cnt:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9YdP8kNNlqO",
        "outputId": "e629e9e0-85d7-4f27-e912-eaa7fab2672d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GloVe embeddings? (y/n): n\n",
            "Use Hierarchical Encoder? (y/n): n\n",
            "Use 2‑GRU Decoder? (y/n): n\n",
            "Use Beam Search? (y/n): y\n",
            "Beam width (e.g. 3): 2\n",
            "\n",
            "Config → GloVe: False, Hier: False, Decoder2: False, Beam: True, k=2\n",
            "\n",
            "Vocab size: 54473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-60874a53fac4>:527: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-3-60874a53fac4>:534: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train 1.530 | val 1.102 | 263.4s\n",
            "Epoch 2: train 0.839 | val 0.872 | 260.3s\n",
            "Epoch 3: train 0.600 | val 0.819 | 263.4s\n",
            "Epoch 4: train 0.438 | val 0.821 | 262.5s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}