# Seq2seq-language-models
“Developed a Sequence-to-Sequence (Seq2Seq) model using RNNs and Transformers to generate Wikipedia article titles from body text. Framed as an extreme summarization task, the project involved training deep neural network–based language models, experimenting with different architectures, and improving components for enhanced performance.”
